{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510a2435",
   "metadata": {},
   "source": [
    "# Étape 2 — Supervisé & Deep Learning (ECG 1D)\n",
    "\n",
    "Ce notebook correspond à **l’Étape 2** du projet :\n",
    "- Classification supervisée de signaux ECG\n",
    "- Basé sur ce que tu as réellement : **battements ECG 1D en CSV (PTBDB / MIT-BIH)**  \n",
    "- Modèles : **CNN1D** et **LSTM**\n",
    "- Évaluation : Accuracy, F1-macro, ROC-AUC (si applicable)\n",
    "\n",
    "⚠️ Hypothèses :\n",
    "- `X = toutes les colonnes sauf la dernière`\n",
    "- `y = dernière colonne` (labels)\n",
    "- Pas d’images ECG ni de textes cliniques ici (pipeline cohérent avec Étape 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8243a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cellule 1 — Imports\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as L, Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6c9d4",
   "metadata": {},
   "source": [
    "## 1) Chargement des données (CSV battements ECG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f52ce343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/raphc/Documents/Cours M2/algorithmique supervisé\n",
      "DATA_DIR: /Users/raphc/Documents/Cours M2/algorithmique supervisé/Dataset Projet\n",
      "Existe ? True\n",
      "ptbdb_normal (4046, 188)\n",
      "ptbdb_abnormal (10506, 188)\n",
      "mitbih_train (87554, 188)\n",
      "mitbih_test (21892, 188)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Racine réelle = parent du dossier Projet\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"Dataset Projet\"\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"Existe ?\", DATA_DIR.exists())\n",
    "\n",
    "FILES = {\n",
    "    \"ptbdb_normal\": \"ptbdb_normal.csv\",\n",
    "    \"ptbdb_abnormal\": \"ptbdb_abnormal.csv\",\n",
    "    \"mitbih_train\": \"mitbih_train.csv\",\n",
    "    \"mitbih_test\": \"mitbih_test.csv\",\n",
    "}\n",
    "\n",
    "def load_csv(path):\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "dfs = {}\n",
    "for k, f in FILES.items():\n",
    "    p = DATA_DIR / f\n",
    "    if p.exists():\n",
    "        dfs[k] = load_csv(p)\n",
    "        print(k, dfs[k].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb43224",
   "metadata": {},
   "source": [
    "## 2) Construction de X / y\n",
    "\n",
    "Par défaut :\n",
    "- PTBDB normal + abnormal\n",
    "- Sinon MIT-BIH (train + test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eca3436a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ptbdb_normal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m USE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPTBDB\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# ou \"MITBIH\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m USE \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPTBDB\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 9\u001b[0m     Xn, yn \u001b[38;5;241m=\u001b[39m split_xy(dfs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mptbdb_normal\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     10\u001b[0m     Xa, ya \u001b[38;5;241m=\u001b[39m split_xy(dfs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mptbdb_abnormal\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     11\u001b[0m     X \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([Xn, Xa], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ptbdb_normal'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cellule 3 — Build dataset\n",
    "\n",
    "def split_xy(df):\n",
    "    return df.iloc[:, :-1], df.iloc[:, -1]\n",
    "\n",
    "USE = \"PTBDB\"  # ou \"MITBIH\"\n",
    "\n",
    "if USE == \"PTBDB\":\n",
    "    Xn, yn = split_xy(dfs[\"ptbdb_normal\"])\n",
    "    Xa, ya = split_xy(dfs[\"ptbdb_abnormal\"])\n",
    "    X = pd.concat([Xn, Xa], ignore_index=True)\n",
    "    y = pd.concat([yn, ya], ignore_index=True)\n",
    "else:\n",
    "    Xt, yt = split_xy(dfs[\"mitbih_train\"])\n",
    "    Xv, yv = split_xy(dfs[\"mitbih_test\"])\n",
    "    X = pd.concat([Xt, Xv], ignore_index=True)\n",
    "    y = pd.concat([yt, yv], ignore_index=True)\n",
    "\n",
    "# nettoyage NaN\n",
    "mask = ~np.isnan(X.values).any(axis=1)\n",
    "X = X.loc[mask].values.astype(\"float32\")\n",
    "y = y.loc[mask].values.astype(\"int64\")\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y distribution:\", dict(pd.Series(y).value_counts()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c5b3e7",
   "metadata": {},
   "source": [
    "## 3) Train / Test split + normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4618196",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cellule 4 — Split + scaling\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# reshape pour réseaux 1D : (N, T, 1)\n",
    "X_train_nn = X_train[..., None]\n",
    "X_test_nn = X_test[..., None]\n",
    "\n",
    "print(\"Train:\", X_train_nn.shape, \"Test:\", X_test_nn.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0dbde",
   "metadata": {},
   "source": [
    "## 4) Modèle CNN 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682929e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cellule 5 — CNN1D\n",
    "\n",
    "def build_cnn1d(input_shape, n_classes):\n",
    "    inp = L.Input(shape=input_shape)\n",
    "    x = L.Conv1D(32, 5, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = L.MaxPool1D(2)(x)\n",
    "    x = L.Conv1D(64, 5, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = L.MaxPool1D(2)(x)\n",
    "    x = L.Conv1D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = L.GlobalAveragePooling1D()(x)\n",
    "    x = L.Dense(64, activation=\"relu\")(x)\n",
    "    out = L.Dense(n_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inp, out)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "n_classes = len(np.unique(y))\n",
    "cnn = build_cnn1d(X_train_nn.shape[1:], n_classes)\n",
    "cnn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5d6876",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cellule 6 — Entraînement CNN\n",
    "\n",
    "hist_cnn = cnn.fit(\n",
    "    X_train_nn, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=15,\n",
    "    batch_size=256,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02be6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cellule 7 — Évaluation CNN\n",
    "\n",
    "y_pred = np.argmax(cnn.predict(X_test_nn), axis=1)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 macro:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf233c5",
   "metadata": {},
   "source": [
    "## 5) Modèle LSTM 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cellule 8 — LSTM\n",
    "\n",
    "def build_lstm(input_shape, n_classes):\n",
    "    inp = L.Input(shape=input_shape)\n",
    "    x = L.LSTM(64, return_sequences=True)(inp)\n",
    "    x = L.LSTM(64)(x)\n",
    "    x = L.Dense(64, activation=\"relu\")(x)\n",
    "    out = L.Dense(n_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inp, out)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "lstm = build_lstm(X_train_nn.shape[1:], n_classes)\n",
    "lstm.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c80fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cellule 9 — Entraînement LSTM\n",
    "\n",
    "hist_lstm = lstm.fit(\n",
    "    X_train_nn, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=15,\n",
    "    batch_size=256,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc1e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cellule 10 — Évaluation LSTM\n",
    "\n",
    "y_pred_lstm = np.argmax(lstm.predict(X_test_nn), axis=1)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lstm))\n",
    "print(\"F1 macro:\", f1_score(y_test, y_pred_lstm, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_lstm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4a5fa9",
   "metadata": {},
   "source": [
    "## 6) Comparaison des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e514a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cellule 11 — Tableau récapitulatif\n",
    "\n",
    "results = pd.DataFrame([\n",
    "    {\n",
    "        \"model\": \"CNN1D\",\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"f1_macro\": f1_score(y_test, y_pred, average=\"macro\"),\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"LSTM\",\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred_lstm),\n",
    "        \"f1_macro\": f1_score(y_test, y_pred_lstm, average=\"macro\"),\n",
    "    },\n",
    "])\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c1b67",
   "metadata": {},
   "source": [
    "## 7) Conclusion (à reporter dans le rapport)\n",
    "\n",
    "- Modèles testés : CNN1D, LSTM  \n",
    "- Données : battements ECG (CSV)  \n",
    "- Split : 80/20 stratifié  \n",
    "- Métriques : Accuracy, F1-macro  \n",
    "- Modèle retenu : celui maximisant F1-macro  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
